{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enabling GPU (allow grow to let cuDNN function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract a previously trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract VGG16 from tensorflow.keras.applications library\n",
    "prior_model = VGG16(weights='imagenet',include_top=False, input_shape=(64,64,3))\n",
    "    # include_top=False : remove the last layer \n",
    "    # input_shape=(64,64,3) : specify input shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building new model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating new model based on previous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(prior_model) # add VGG16 as a typical layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary() # VGG16 is considered as one single layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.layers[0].summary() # here to get details about VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Freezing layers from previous model (total freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in model.layers[0].layers: # looping over each layers in layer 0 to freeze them\n",
    "    layers.trainable = False\n",
    "\n",
    "model.layers[0].trainable = False # freezing layer 0 as well for good measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compiling the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building new model 2 (higher resolution 128 x 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading previous model\n",
    "PREVIOUS_MODEL_PATH = '../../_out_of_git/Deeplearning/64_by_64.h5'\n",
    "previous_model = load_model(PREVIOUS_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_model.layers[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model HR (higher resolution)\n",
    "modelHR = Sequential()\n",
    "modelHR.add(Conv2D(64,kernel_size=(3,3),input_shape=(128,128,3),activation='relu', padding='same'))\n",
    "modelHR.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "for layer in previous_model.layers[0].layers[2:]:\n",
    "  # here we precise that we want to take all the layers from the second one to the last one in the VGG-16 model.\n",
    "  modelHR.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x252a3d26340>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x252ac1f39a0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x252b26be640>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x252b39aa250>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_model.layers[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we add all the other layers in our previous model, while freezing them in the process\n",
    "for layer in previous_model.layers[1:]:\n",
    "  layer.trainable = False\n",
    "  modelHR.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 15,239,746\n",
      "Trainable params: 1,792\n",
      "Non-trainable params: 15,237,954\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelHR.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling model HR\n",
    "modelHR.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating image generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# creating the image generator\n",
    "\n",
    "generator = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    rescale=1/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "test_generator = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating train test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1589 images belonging to 2 classes.\n",
      "Found 378 images belonging to 2 classes.\n",
      "Found 1589 images belonging to 2 classes.\n",
      "Found 378 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# defining the constants for the model training\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "URL_TRAINING = '../../_out_of_git/Deeplearning/CatdogData/training_set' \n",
    "URL_TESTING = '../../_out_of_git/Deeplearning/CatdogData/test_set' \n",
    "\n",
    "# creating the train and test sets\n",
    "\n",
    "train_set = generator.flow_from_directory(URL_TRAINING, target_size=(64,64), batch_size=BATCH_SIZE)\n",
    "test_set = test_generator.flow_from_directory(URL_TESTING, target_size=(64,64), batch_size=BATCH_SIZE)\n",
    "\n",
    "# creating the train and test sets with HR (128 x 128)\n",
    "\n",
    "train_setHR = generator.flow_from_directory(URL_TRAINING, target_size=(128,128), batch_size=BATCH_SIZE)\n",
    "test_setHR = test_generator.flow_from_directory(URL_TESTING, target_size=(128,128), batch_size=BATCH_SIZE)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcmeu\\Anaconda3\\envs\\GPU-enabled\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "49/49 [==============================] - 22s 230ms/step - loss: 0.8656 - accuracy: 0.5733 - val_loss: 0.5658 - val_accuracy: 0.7102\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 3s 54ms/step - loss: 0.5498 - accuracy: 0.7146 - val_loss: 0.5007 - val_accuracy: 0.7358\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 3s 54ms/step - loss: 0.4945 - accuracy: 0.7601 - val_loss: 0.4878 - val_accuracy: 0.7614\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4851 - accuracy: 0.7607 - val_loss: 0.4927 - val_accuracy: 0.7756\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 3s 54ms/step - loss: 0.4929 - accuracy: 0.7391 - val_loss: 0.4993 - val_accuracy: 0.7528\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 3s 54ms/step - loss: 0.4689 - accuracy: 0.7845 - val_loss: 0.5280 - val_accuracy: 0.7614\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4893 - accuracy: 0.7622 - val_loss: 0.5306 - val_accuracy: 0.7415\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4484 - accuracy: 0.7862 - val_loss: 0.5277 - val_accuracy: 0.7415\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 3s 54ms/step - loss: 0.4936 - accuracy: 0.7551 - val_loss: 0.5164 - val_accuracy: 0.7500\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4770 - accuracy: 0.7487 - val_loss: 0.4766 - val_accuracy: 0.7500\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 3s 57ms/step - loss: 0.4431 - accuracy: 0.7909 - val_loss: 0.4808 - val_accuracy: 0.7585\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 3s 56ms/step - loss: 0.4504 - accuracy: 0.7907 - val_loss: 0.5358 - val_accuracy: 0.7386\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4586 - accuracy: 0.7814 - val_loss: 0.5045 - val_accuracy: 0.7188\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 3s 56ms/step - loss: 0.4288 - accuracy: 0.7988 - val_loss: 0.4601 - val_accuracy: 0.7699\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 3s 56ms/step - loss: 0.4487 - accuracy: 0.7750 - val_loss: 0.4550 - val_accuracy: 0.7670\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4268 - accuracy: 0.8032 - val_loss: 0.4690 - val_accuracy: 0.7727\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4521 - accuracy: 0.7631 - val_loss: 0.5595 - val_accuracy: 0.7216\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 3s 55ms/step - loss: 0.4886 - accuracy: 0.7778 - val_loss: 0.4689 - val_accuracy: 0.7756\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 3s 56ms/step - loss: 0.4402 - accuracy: 0.7928 - val_loss: 0.4754 - val_accuracy: 0.7443\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 3s 54ms/step - loss: 0.4048 - accuracy: 0.8098 - val_loss: 0.4915 - val_accuracy: 0.7443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18a31006100>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit_generator(train_set, steps_per_epoch=len(train_set.filenames)//BATCH_SIZE, epochs=EPOCHS, validation_data = test_set, validation_steps=len(test_set.filenames)//BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "49/49 [==============================] - 11s 138ms/step - loss: 0.6967 - accuracy: 0.5074 - val_loss: 0.6806 - val_accuracy: 0.5511\n",
      "Epoch 2/20\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 0.6957 - accuracy: 0.5163 - val_loss: 0.6634 - val_accuracy: 0.5938\n",
      "Epoch 3/20\n",
      "49/49 [==============================] - 6s 121ms/step - loss: 0.6735 - accuracy: 0.5769 - val_loss: 0.6642 - val_accuracy: 0.5852\n",
      "Epoch 4/20\n",
      "49/49 [==============================] - 6s 119ms/step - loss: 0.6861 - accuracy: 0.5515 - val_loss: 0.6510 - val_accuracy: 0.6250\n",
      "Epoch 5/20\n",
      "49/49 [==============================] - 6s 116ms/step - loss: 0.6733 - accuracy: 0.5826 - val_loss: 0.6546 - val_accuracy: 0.6278\n",
      "Epoch 6/20\n",
      "49/49 [==============================] - 6s 114ms/step - loss: 0.6877 - accuracy: 0.5634 - val_loss: 0.6332 - val_accuracy: 0.6506\n",
      "Epoch 7/20\n",
      "49/49 [==============================] - 6s 115ms/step - loss: 0.6659 - accuracy: 0.6001 - val_loss: 0.6268 - val_accuracy: 0.6335\n",
      "Epoch 8/20\n",
      "49/49 [==============================] - 6s 116ms/step - loss: 0.6541 - accuracy: 0.6156 - val_loss: 0.6285 - val_accuracy: 0.6335\n",
      "Epoch 9/20\n",
      "49/49 [==============================] - 6s 115ms/step - loss: 0.6366 - accuracy: 0.6294 - val_loss: 0.6193 - val_accuracy: 0.6534\n",
      "Epoch 10/20\n",
      "49/49 [==============================] - 6s 115ms/step - loss: 0.6370 - accuracy: 0.6416 - val_loss: 0.6196 - val_accuracy: 0.6591\n",
      "Epoch 11/20\n",
      "49/49 [==============================] - 6s 115ms/step - loss: 0.6477 - accuracy: 0.6079 - val_loss: 0.6053 - val_accuracy: 0.6875\n",
      "Epoch 12/20\n",
      "49/49 [==============================] - 6s 115ms/step - loss: 0.6290 - accuracy: 0.6407 - val_loss: 0.5999 - val_accuracy: 0.6733\n",
      "Epoch 13/20\n",
      "49/49 [==============================] - 6s 115ms/step - loss: 0.6123 - accuracy: 0.6714 - val_loss: 0.5928 - val_accuracy: 0.6875\n",
      "Epoch 14/20\n",
      "49/49 [==============================] - 6s 115ms/step - loss: 0.6296 - accuracy: 0.6310 - val_loss: 0.5946 - val_accuracy: 0.6591\n",
      "Epoch 15/20\n",
      "49/49 [==============================] - 6s 117ms/step - loss: 0.6149 - accuracy: 0.6572 - val_loss: 0.5955 - val_accuracy: 0.6818\n",
      "Epoch 16/20\n",
      "49/49 [==============================] - 6s 117ms/step - loss: 0.6144 - accuracy: 0.6666 - val_loss: 0.5966 - val_accuracy: 0.6790\n",
      "Epoch 17/20\n",
      "49/49 [==============================] - 6s 117ms/step - loss: 0.6098 - accuracy: 0.6690 - val_loss: 0.5876 - val_accuracy: 0.6790\n",
      "Epoch 18/20\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 0.6114 - accuracy: 0.6545 - val_loss: 0.5963 - val_accuracy: 0.6705\n",
      "Epoch 19/20\n",
      "49/49 [==============================] - 6s 117ms/step - loss: 0.6016 - accuracy: 0.6628 - val_loss: 0.5845 - val_accuracy: 0.6960\n",
      "Epoch 20/20\n",
      "49/49 [==============================] - 6s 117ms/step - loss: 0.6050 - accuracy: 0.6746 - val_loss: 0.5737 - val_accuracy: 0.6989\n"
     ]
    }
   ],
   "source": [
    "# fitting the model with HR (128 x 128)\n",
    "with tf.device('/GPU:0'):\n",
    "    modelHR.fit_generator(train_setHR, steps_per_epoch=len(train_setHR.filenames)//BATCH_SIZE, epochs=EPOCHS, validation_data = test_setHR, validation_steps=len(test_setHR.filenames)//BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.177306e-20 1.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "TEST_IMAGE_URL = './test_image_cat.jpg'\n",
    "\n",
    "test_image = image.load_img( TEST_IMAGE_URL , target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "TEST_IMAGE_URL1 = './test_image_dog.jpg'\n",
    "\n",
    "test_image1 = image.load_img( TEST_IMAGE_URL1 , target_size = (64, 64))\n",
    "test_image1 = image.img_to_array(test_image1)\n",
    "test_image1 = np.expand_dims(test_image1, axis = 0)\n",
    "result1 = model.predict(test_image1)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcmeu\\Anaconda3\\envs\\GPU-enabled\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1877: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4828740954399109, 0.7486772537231445]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.evaluate(test_set, steps=BATCH_SIZE)\n",
    "model.evaluate_generator(test_set, steps=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5755034685134888, 0.7010582089424133]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelHR.evaluate_generator(test_setHR, steps=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.preprocessing.image.DirectoryIterator at 0x18aad8ced30>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_setHR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to save the model\n",
    "\n",
    "PATH = '../../_out_of_git/Deeplearning/64_by_64.h5'\n",
    "\n",
    "model.save(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU-enabled",
   "language": "python",
   "name": "gpu-enabled"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
